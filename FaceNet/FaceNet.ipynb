{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n",
      "[INFO] extracting only face from the image...\n",
      "[INFO] expand dimensions...\n",
      "[INFO] preprocess image...\n",
      "[INFO] loading model...\n",
      "[INFO] making prediction...\n",
      "[INFO] decoding predictions...\n",
      "b' Sharon_Stone': 99.574%\n",
      "b' Noelle_Reno': 0.080%\n",
      "b' Anita_Lipnicka': 0.027%\n",
      "b' Elisabeth_R\\xc3\\xb6hm': 0.027%\n",
      "b' Emma_Atkins': 0.019%\n"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    print(\"[INFO] extracting only face from the image...\")\n",
    "    pixels = plt.imread(filename)\n",
    "    detector = MTCNN()\n",
    "    results = detector.detect_faces(pixels)\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    " \n",
    "print(\"[INFO] loading image...\")\n",
    "pixels = extract_face('sharon_stone1.jpg')\n",
    "pixels = pixels.astype('float32')\n",
    "\n",
    "print(\"[INFO] expand dimensions...\")\n",
    "samples = expand_dims(pixels, axis=0)\n",
    "\n",
    "print(\"[INFO] preprocess image...\")\n",
    "samples = preprocess_input(samples, version=2)\n",
    "\n",
    "print(\"[INFO] loading model...\")\n",
    "model = VGGFace(model='resnet50')\n",
    "\n",
    "print(\"[INFO] making prediction...\")\n",
    "yhat = model.predict(samples)\n",
    "\n",
    "print(\"[INFO] decoding predictions...\")\n",
    "results = decode_predictions(yhat)\n",
    "for result in results[0]:\n",
    "    print('%s: %.3f%%' % (str(result[0].encode('utf-8').decode('utf-8')), result[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('labels.json',\"r+\") as f:\n",
    "    l = f.read()\n",
    "\n",
    "data = json.loads(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n",
      "[INFO] extracting only face from the image...\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6ef43c3440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6ef43c3440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[INFO] preprocessing image...\n",
      "[INFO] init consta and variablese...\n",
      "[INFO] generating adversarials watching delta...\n",
      "[INFO] step: 0, loss: -0.006777158007025719...\n",
      "[INFO] step: 5, loss: -0.10115277767181396...\n",
      "[INFO] step: 10, loss: -2.4546256065368652...\n",
      "[INFO] step: 15, loss: -7.9793829917907715...\n",
      "[INFO] step: 20, loss: -12.94911003112793...\n",
      "[INFO] step: 25, loss: -16.84220314025879...\n",
      "[INFO] step: 30, loss: -19.839214324951172...\n",
      "[INFO] step: 35, loss: -22.07464599609375...\n",
      "[INFO] step: 40, loss: -23.84256362915039...\n",
      "[INFO] step: 45, loss: -25.312740325927734...\n",
      "[INFO] creating adversarial example...\n",
      "[INFO] saving the adversarial example...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "def clip_eps(tensor, eps):\n",
    "    return tf.clip_by_value(tensor, clip_value_min=-eps, clip_value_max=eps)\n",
    "\n",
    "\n",
    "def generate_adversaries(model, baseImage, delta, classIdx, steps=50):\n",
    "    for step in range(0, steps):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            tape.watch(delta)\n",
    "\n",
    "            adversary = preprocess_input(baseImage + delta)\n",
    "            predictions = model(adversary, training=False)\n",
    "            loss = -sccLoss(tf.convert_to_tensor([classIdx]), predictions)\n",
    "            \n",
    "            if step % 5 == 0:\n",
    "                print(\"[INFO] step: {}, loss: {}...\".format(step,\n",
    "                    loss.numpy()))\n",
    "        \n",
    "            gradients = tape.gradient(loss, delta)\n",
    "            optimizer.apply_gradients([(gradients, delta)])\n",
    "            delta = delta.assign_add(clip_eps(delta, eps=EPS))\n",
    "    return delta\n",
    "\n",
    "EPS = 2 / 255\n",
    "LR = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "sccLoss = SparseCategoricalCrossentropy()\n",
    "\n",
    "print(\"[INFO] loading image...\")\n",
    "pixels = extract_face('sharon_stone1.jpg')\n",
    "pixels = pixels.astype('float32')\n",
    "\n",
    "print(\"[INFO] preprocessing image...\")\n",
    "image = preprocess_image(pixels)\n",
    "\n",
    "print(\"[INFO] init consta and variablese...\")\n",
    "baseImage = tf.constant(image, dtype=tf.float32)\n",
    "delta = tf.Variable(tf.zeros_like(baseImage), trainable=True)\n",
    "\n",
    "print(\"[INFO] generating adversarials watching delta...\")\n",
    "deltaUpdated = generate_adversaries(model, baseImage, delta, int(data[' Sharon_Stone']))\n",
    "\n",
    "print(\"[INFO] creating adversarial example...\")\n",
    "adverImage = (baseImage + deltaUpdated).numpy().squeeze()\n",
    "adverImage = np.clip(adverImage, 0, 255).astype(\"uint8\")\n",
    "adverImage = cv2.cvtColor(adverImage, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "print(\"[INFO] saving the adversarial example...\")\n",
    "cv2.imwrite(\"adverImage.png\", adverImage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] reading adversarial image example...\n",
      "[INFO] preprocessing adversarial image example...\n",
      "[INFO] making predictions with adversarial image example...\n",
      "[INFO] decoding predictions with adversarial image example...\n",
      "b' Bobbi_Sue_Luther': 99.998%\n",
      "b' Sam_Pinto': 0.000%\n",
      "b' Maura_Rivera': 0.000%\n",
      "b' Georgina_Verbaan': 0.000%\n",
      "b' Agnieszka_Maciag': 0.000%\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] reading adversarial image example...\")\n",
    "image = cv2.imread(\"adverImage.png\")\n",
    "\n",
    "print(\"[INFO] preprocessing adversarial image example...\")\n",
    "image = preprocess_image(image)\n",
    "\n",
    "preprocessedImage = preprocess_input(image)\n",
    "\n",
    "print(\"[INFO] making predictions with adversarial image example...\")\n",
    "predictions = model.predict(preprocessedImage)\n",
    "\n",
    "print(\"[INFO] decoding predictions with adversarial image example...\")\n",
    "predictions = decode_predictions(predictions)\n",
    "\n",
    "for result in predictions[0]:\n",
    "    print('%s: %.3f%%' % (str(result[0].encode('utf-8').decode('utf-8')), result[1]*100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
