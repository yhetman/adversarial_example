{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract face ...\n",
      "Expand dims ...\n",
      "Preprocess image ...\n",
      "Loading model ...\n",
      "Prediction ...\n",
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_labels_v2.npy\n",
      "1351680/1346516 [==============================] - 0s 0us/step\n",
      "1359872/1346516 [==============================] - 0s 0us/step\n",
      "b' Sharon_Stone': 99.574%\n",
      "b' Noelle_Reno': 0.080%\n",
      "b' Anita_Lipnicka': 0.027%\n",
      "b' Elisabeth_R\\xc3\\xb6hm': 0.027%\n",
      "b' Emma_Atkins': 0.019%\n"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    # load image from file\n",
    "    pixels = plt.imread(filename)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    " \n",
    "# load the photo and extract the face\n",
    "print(\"Extract face ...\")\n",
    "\n",
    "pixels = extract_face('sharon_stone1.jpg')\n",
    "# plot the extracted face\n",
    "# convert one face into samples\n",
    "pixels = pixels.astype('float32')\n",
    "print(\"Expand dims ...\")\n",
    "\n",
    "samples = expand_dims(pixels, axis=0)\n",
    "print(\"Preprocess image ...\")\n",
    "\n",
    "# prepare the face for the model, e.g. center pixels\n",
    "samples = preprocess_input(samples, version=2)\n",
    "# create a vggface model\n",
    "print(\"Loading model ...\")\n",
    "\n",
    "model = VGGFace(model='resnet50')\n",
    "# perform prediction\n",
    "print(\"Prediction ...\")\n",
    "\n",
    "yhat = model.predict(samples)\n",
    "# convert prediction into names\n",
    "results = decode_predictions(yhat)\n",
    "# display most likely results\n",
    "for result in results[0]:\n",
    "    print('%s: %.3f%%' % (result[0], result[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Downloads/rcmalli_vggface_labels_v2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1eef2c3aa1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Downloads/rcmalli_vggface_labels_v2.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yhetman/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Downloads/rcmalli_vggface_labels_v2.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('~/Downloads/rcmalli_vggface_labels_v2.npy')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading image...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-32596f409d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] loading image...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # swap color channels, resize the input image, and add a batch\n",
    "    # dimension\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # return the preprocessed image\n",
    "    return image\n",
    "\n",
    "def clip_eps(tensor, eps):\n",
    "    # clip the values of the tensor to a given range and return it\n",
    "    return tf.clip_by_value(tensor, clip_value_min=-eps,\n",
    "        clip_value_max=eps)\n",
    "\n",
    "\n",
    "def generate_adversaries(model, baseImage, delta, classIdx, steps=50):\n",
    "    # iterate over the number of steps\n",
    "    for step in range(0, steps):\n",
    "        # record our gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # explicitly indicate that our perturbation vector should\n",
    "            # be tracked for gradient updates\n",
    "            tape.watch(delta)\n",
    "            # add our perturbation vector to the base image and\n",
    "            # preprocess the resulting image\n",
    "            adversary = preprocess_input(baseImage + delta)\n",
    "            # run this newly constructed image tensor through our\n",
    "            # model and calculate the loss with respect to the\n",
    "            # *original* class index\n",
    "            predictions = model(adversary, training=False)\n",
    "            loss = -sccLoss(tf.convert_to_tensor([classIdx]),\n",
    "                predictions)\n",
    "            # check to see if we are logging the loss value, and if\n",
    "            # so, display it to our terminal\n",
    "            if step % 5 == 0:\n",
    "                print(\"step: {}, loss: {}...\".format(step,\n",
    "                    loss.numpy()))\n",
    "        # calculate the gradients of loss with respect to the\n",
    "        # perturbation vector\n",
    "        gradients = tape.gradient(loss, delta)\n",
    "        # update the weights, clip the perturbation vector, and\n",
    "        # update its value\n",
    "        optimizer.apply_gradients([(gradients, delta)])\n",
    "        delta.assign_add(clip_eps(delta, eps=EPS))\n",
    "    # return the perturbation vector\n",
    "    return delta\n",
    "\n",
    "EPS = 2 / 255.0\n",
    "LR = 0.1\n",
    "optimizer = Adam(learning_rate=LR)\n",
    "sccLoss = SparseCategoricalCrossentropy()\n",
    "# load the input image from disk and preprocess it\n",
    "print(\"[INFO] loading image...\")\n",
    "samples = expand_dims(pixels, axis=0)\n",
    "image = preprocess_image(samples)\n",
    "\n",
    "baseImage = tf.constant(image, dtype=tf.float32)\n",
    "delta = tf.Variable(tf.zeros_like(baseImage), trainable=True)\n",
    "# generate the perturbation vector to create an adversarial example\n",
    "print(\"[INFO] generating perturbation...\")\n",
    "deltaUpdated = generate_adversaries(model, baseImage, delta,\n",
    "\targs[\"class_idx\"])\n",
    "# create the adversarial example, swap color channels, and save the\n",
    "# output image to disk\n",
    "print(\"[INFO] creating adversarial example...\")\n",
    "adverImage = (baseImage + deltaUpdated).numpy().squeeze()\n",
    "adverImage = np.clip(adverImage, 0, 255).astype(\"uint8\")\n",
    "adverImage = cv2.cvtColor(adverImage, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(args[\"output\"], adverImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
